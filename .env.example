# ============================================
# Plugin Architecture Configuration (MVP 5)
# ============================================
# LLM Provider (ollama|openai|anthropic)
LLM_PROVIDER=ollama

# LLM Model (provider-specific)
LLM_MODEL=llama3.1:latest

# Embedding Provider (huggingface)
EMBEDDING_PROVIDER=huggingface

# Embedding Model
EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5

# Retrieval Strategy (hybrid|semantic|keyword)
RETRIEVAL_STRATEGY=hybrid

# ============================================
# LLM Provider API Keys
# ============================================
# Anthropic API key (optional - for Claude models)
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Google Gemini API key (optional - for Gemini models)
GOOGLE_API_KEY=your_google_api_key_here

# OpenAI API key (optional - for OpenAI models)
OPENAI_API_KEY=your_openai_api_key_here

# ============================================
# Model Configuration (configurable defaults)
# ============================================
# Default LLM model to use on startup (legacy - use LLM_MODEL above)
DEFAULT_LLM_MODEL=llama3.1:latest

# Default embedding model (legacy - use EMBEDDING_MODEL above)
DEFAULT_EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5

# ============================================
# Model Provider Settings
# ============================================
# Anthropic (Claude) settings
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
ANTHROPIC_TEMPERATURE=0.7
ANTHROPIC_MAX_TOKENS=4096

# Gemini settings
GEMINI_MODEL=gemini-2.0-flash-exp
GEMINI_TEMPERATURE=0.7
GEMINI_MAX_OUTPUT_TOKENS=4096

# OpenAI settings
OPENAI_MODEL=gpt-4-turbo
OPENAI_TEMPERATURE=0.7

# Ollama settings
OLLAMA_HOST=localhost
OLLAMA_PORT=11434

# ============================================
# Context Window Configuration
# ============================================
# Ollama model context window (how much the model can "see")
# llama3.1 supports up to 128K, default is 128000
CONTEXT_WINDOW=128000

# Chat memory buffer limit (for conversation history + retrieved context)
# This limits the tokens used for chat memory, default is 32000
# Increase for longer conversations, decrease for faster responses
CHAT_TOKEN_LIMIT=32000

# ============================================
# Image Generation Configuration (Imagen / Gemini)
# ============================================
# Image generation provider (gemini)
IMAGE_GENERATION_PROVIDER=gemini

# Gemini/Imagen Image Generation settings (uses GOOGLE_API_KEY from above)
# Models (Imagen):
#   imagen-4.0-generate-001 - Production quality (default)
#   imagen-4.0-fast-generate-001 - Faster, lower quality
#   imagen-4.0-ultra-generate-001 - Highest quality
# Models (Gemini/Nano Banana):
#   gemini-2.0-flash-exp - Nano Banana, fast experimental
#   gemini-3-pro-image-preview - Nano Banana Pro, higher quality 4K
GEMINI_IMAGE_MODEL=imagen-4.0-generate-001

# Output directory for generated images
IMAGE_OUTPUT_DIR=outputs/images

# Maximum image file size in MB
MAX_IMAGE_SIZE_MB=10

# Supported image formats (comma-separated)
SUPPORTED_IMAGE_FORMATS=jpg,jpeg,png,webp

# ============================================
# Vision Model Configuration (Image Understanding)
# ============================================
# Enable vision models for image processing (true|false)
USE_VISION_FOR_IMAGES=true

# Vision provider for image understanding (gemini|openai)
VISION_PROVIDER=gemini

# Gemini Vision model (uses GOOGLE_API_KEY from above)
GEMINI_VISION_MODEL=gemini-2.0-flash-exp

# OpenAI Vision model (uses OPENAI_API_KEY from above)
OPENAI_VISION_MODEL=gpt-4o

# ============================================
# Document Management
# ============================================
# Persist documents in vector DB (true|false)
PERSIST_DOCUMENTS=true

# Maximum documents per user/session
MAX_DOCUMENTS=100

# Document storage path
DOCUMENT_STORAGE_PATH=data/documents

# Auto-activate new documents (true|false)
AUTO_ACTIVATE_DOCUMENTS=false

# ============================================
# Advanced Retrieval Configuration
# ============================================
# Contextual Retrieval (Anthropic approach)
# Enriches chunks with LLM-generated context during ingestion
# Improves retrieval for structured content (tables, lists)
CONTEXTUAL_RETRIEVAL_ENABLED=false
CONTEXTUAL_BATCH_SIZE=5
CONTEXTUAL_MAX_CONCURRENCY=3
CONTEXTUAL_MAX_CHUNK_CHARS=2000

# Hybrid Retrieval (BM25 + Vector)
# Already enabled by default in RAPTOR retriever
# Weights configured in raptor/config.py (bm25_weight=0.4, vector_weight=0.6)

# ============================================
# Web Content Integration (Firecrawl + Jina)
# ============================================
# Firecrawl API key for web search (required for search feature)
FIRECRAWL_API_KEY=your_firecrawl_api_key_here

# Jina API key for higher rate limits (optional)
# Without API key: 20 RPM, With API key: 200 RPM
JINA_API_KEY=your_jina_api_key_here

# ============================================
# PostgreSQL Configuration
# ============================================
POSTGRES_HOST=localhost
POSTGRES_PORT=5433
POSTGRES_DB=dbnotebook_dev
POSTGRES_USER=postgres
POSTGRES_PASSWORD=root

# Full DATABASE_URL (alternative to individual settings)
# DATABASE_URL=postgresql://postgres:root@localhost:5433/dbnotebook_dev
