# ============================================
# API Keys
# ============================================
# Anthropic API key (optional - for Claude models)
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Google Gemini API key (for Gemini models, vision, and image generation)
GOOGLE_API_KEY=your_google_api_key_here

# OpenAI API key (for GPT models and embeddings)
OPENAI_API_KEY=your_openai_api_key_here

# Groq API key (for ultra-fast LLM inference) - https://groq.com/
GROQ_API_KEY=your_groq_api_key_here

# Tavily API key (for web search and scraping) - https://tavily.com/
TAVILY_API_KEY=your_tavily_api_key_here

# ============================================
# LLM Provider Configuration
# ============================================
# Provider options: ollama, openai, anthropic, gemini, groq
LLM_PROVIDER=ollama
LLM_MODEL=llama3.1:latest

# Groq models (when LLM_PROVIDER=groq):
#   meta-llama/llama-4-maverick-17b-128e-instruct (recommended)
#   meta-llama/llama-4-scout-17b-16e-instruct (faster, fewer experts)
#   llama-3.3-70b-versatile (stable, proven)
#   llama-3.1-8b-instant (fastest, least accurate)
#   openai/gpt-oss-120b (best quality, Apache 2.0 licensed)
#   openai/gpt-oss-20b (fast, Apache 2.0 licensed)
GROQ_MODEL=meta-llama/llama-4-maverick-17b-128e-instruct

# ============================================
# Embedding Configuration
# ============================================
EMBEDDING_PROVIDER=openai
# Embedding model - must match PGVECTOR_EMBED_DIM below
# OpenAI: text-embedding-3-small (1536-dim), text-embedding-3-large (3072-dim)
# HuggingFace: nomic-ai/nomic-embed-text-v1.5 (768-dim)
EMBEDDING_MODEL=text-embedding-3-small
RETRIEVAL_STRATEGY=hybrid

# ============================================
# Reranker Configuration
# ============================================
# LOCAL MODELS (CPU cross-encoder, requires download):
#   xsmall (~22s on 4-core CPU), base (~30s), large (~60s), disabled
#
# GROQ LLM RERANKERS (ultra-fast cloud, requires GROQ_API_KEY):
#   groq:scout (~300ms, recommended for speed)
#   groq:maverick (~400ms, better quality)
#   groq:llama70b (~600ms, very good)
#   groq:gpt-oss (~600ms, best accuracy)
#
# RAG/API uses this for query reranking (set "disabled" for fastest responses)
RERANKER_MODEL=base
# SQL Chat can use a separate reranker (more accurate for few-shot retrieval)
SQL_RERANKER_MODEL=base

# ============================================
# Model Settings (temperature/tokens)
# ============================================
# Anthropic (Claude) settings
ANTHROPIC_TEMPERATURE=0.7
ANTHROPIC_MAX_TOKENS=4096

# Gemini settings
GEMINI_TEMPERATURE=0.7
GEMINI_MAX_OUTPUT_TOKENS=4096

# ============================================
# Ollama
# ============================================
OLLAMA_HOST=localhost
OLLAMA_PORT=11434

# ============================================
# Context Window
# ============================================
# Model context window (llama3.1 supports 128K)
CONTEXT_WINDOW=128000
# Chat memory buffer limit
CHAT_TOKEN_LIMIT=32000

# ============================================
# Image Generation / Studio
# ============================================
IMAGE_GENERATION_PROVIDER=gemini
# Gemini image models:
#   gemini-3-pro-image-preview - Nano Banana Pro, higher quality 4K
#   gemini-2.0-flash-exp - Nano Banana, fast experimental
# Imagen models:
#   imagen-4.0-generate-001 - Production quality
#   imagen-4.0-fast-generate-001 - Faster, lower quality
#   imagen-4.0-ultra-generate-001 - Highest quality
GEMINI_IMAGE_MODEL=gemini-3-pro-image-preview
IMAGE_OUTPUT_DIR=outputs/images
MAX_IMAGE_SIZE_MB=10
SUPPORTED_IMAGE_FORMATS=jpg,jpeg,png,webp

# ============================================
# Vision
# ============================================
VISION_PROVIDER=gemini
GEMINI_VISION_MODEL=gemini-2.0-flash-exp
OPENAI_VISION_MODEL=gpt-4o

# ============================================
# Database (PostgreSQL + pgvector)
# ============================================
# LOCAL DEV:  use localhost
# DOCKER:     use host.docker.internal (to reach host's PostgreSQL)
# REMOTE:     use actual hostname (e.g., your-db.cloud.com)
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=dbnotebook_dev
POSTGRES_USER=dbnotebook
POSTGRES_PASSWORD=dbnotebook
DATABASE_URL=postgresql://dbnotebook:dbnotebook@localhost:5432/dbnotebook_dev

# ============================================
# pgvector
# ============================================
# Table name for vector embeddings (must match alembic migration)
PGVECTOR_TABLE_NAME=data_embeddings
# Embedding dimension (1536 for OpenAI text-embedding-3-small)
PGVECTOR_EMBED_DIM=1536

# ============================================
# SQL Chat
# ============================================
# Encryption key for stored database connections (leave empty for dev)
# Generate with: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
SQL_CHAT_ENCRYPTION_KEY=
# Skip readonly connection check (for databases that don't support it)
SQL_CHAT_SKIP_READONLY_CHECK=false
# Maximum few-shot examples for SQL generation
FEW_SHOT_MAX_EXAMPLES=100000

# ============================================
# API Authentication (optional)
# ============================================
# API key for /api/query endpoint (programmatic access)
# When set, requests must include X-API-Key header
API_KEY=

# ============================================
# RBAC Configuration (optional)
# ============================================
# Enable strict role-based access control
# When true, enforces user permissions on all endpoints
RBAC_STRICT_MODE=false
